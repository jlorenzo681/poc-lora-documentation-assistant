# Local LLM Provider (mlx is default for Apple Silicon, lmstudio as alternative)
LLM_PROVIDER=mlx

# Base URL for LM Studio (local server)
# Use http://host.docker.internal:1234/v1 for Docker users on Mac/Windows
# Use http://localhost:1234/v1 for native execution
LLM_BASE_URL=http://host.docker.internal:8080/v1

# MLX Model Path (if using LLM_PROVIDER=mlx)
# This can be a HuggingFace repo ID or a local path
MLX_MODEL_PATH=mlx-community/Mistral-7B-Instruct-v0.3-4bit
MLX_ADAPTER_PATH=data/adapters

# Langfuse Observability
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_HOST=http://localhost:3000

# Langfuse Server Internals (NextAuth & Database)
# These are used by the self-hosted server backend
NEXTAUTH_SECRET=mysecret_change_me
SALT=mysalt_change_me
NEXTAUTH_URL=http://localhost:3000
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres

# Cloud Storage Connectors (Optional)
# Required only if using OneDrive or Google Drive connectors
MICROSOFT_CLIENT_ID=your_microsoft_client_id_here
MICROSOFT_CLIENT_SECRET=your_microsoft_client_secret_here
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here

# API Base URL (for Admin UI)
API_BASE_URL=http://localhost:8001
